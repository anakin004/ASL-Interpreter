{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some dependensies and version matching\n",
    "#pip install tensorflow-cpu==2.12.0 opencv-python mediapipe==0.10.5 scikit-learn matplotlib\n",
    "#pip install ipykernel\n",
    "#python -m ipykernel install --user --name=env --display-name \"Python (asl_env)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import time\n",
    "import mediapipe as mp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic #holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils #drawing utilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "\n",
    "    if results.left_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(image,results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                                    mp_drawing.DrawingSpec(color=(60,10,10), thickness=1, circle_radius=2),\n",
    "                                    mp_drawing.DrawingSpec(color=(210,206,121), thickness=1, circle_radius=2)\n",
    "                                    )\n",
    "\n",
    "    if results.right_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(image,results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                                        mp_drawing.DrawingSpec(color=(210,200,60), thickness=1, circle_radius=2),\n",
    "                                        mp_drawing.DrawingSpec(color=(10,256,121), thickness=1, circle_radius=2)\n",
    "                                        )\n",
    "    \n",
    "    if results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(image,results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                                    mp_drawing.DrawingSpec(color=(98,13,49), thickness=1, circle_radius=2),\n",
    "                                    mp_drawing.DrawingSpec(color=(109,201,0), thickness=1, circle_radius=2)\n",
    "                                    )\n",
    "    if results.face_landmarks:   \n",
    "        mp_drawing.draw_landmarks(image,results.face_landmarks, mp_holistic.FACEMESH_CONTOURS,\n",
    "                                    mp_drawing.DrawingSpec(color=(98,13,49), thickness=1, circle_radius=2),\n",
    "                                    mp_drawing.DrawingSpec(color=(109,201,0), thickness=1, circle_radius=2)\n",
    "                                    )\n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image,model):\n",
    "    image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB) #converting color\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)  #detection, prediction\n",
    "    image.flags.writeable = True \n",
    "    image = cv2.cvtColor(image,cv2.COLOR_RGB2BGR) #converting back\n",
    "    return image, results\n",
    "\n",
    "# we are grabbing image, converting its format for model, changing write access for memory saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# we want to extract keypoints from hands, pose, face, but if they are not in frame they will\n",
    "# be the same shape but just zeros\n",
    "def extract_keypoints(result):\n",
    "    pose = []\n",
    "    \n",
    "    if result.pose_landmarks:\n",
    "        for res in result.pose_landmarks.landmark:\n",
    "            pose.append(np.array([res.x, res.y, res.z, res.visibility]))\n",
    "        pose = np.array(pose).flatten()\n",
    "    else:\n",
    "        pose = np.zeros(33*4)\n",
    "\n",
    "    rh = []\n",
    "\n",
    "    if result.right_hand_landmarks:\n",
    "        for res in result.right_hand_landmarks.landmark:\n",
    "            rh.append(np.array([res.x, res.y, res.z]))\n",
    "        rh = np.array(rh).flatten()\n",
    "    else:\n",
    "        rh = np.zeros(21*3)\n",
    "\n",
    "    lh = []\n",
    "\n",
    "    if result.left_hand_landmarks:\n",
    "        for res in result.left_hand_landmarks.landmark:\n",
    "            lh.append(np.array([res.x, res.y, res.z]))\n",
    "        lh = np.array(lh).flatten()\n",
    "    else:\n",
    "        lh = np.zeros(21*3)\n",
    "\n",
    "    face = []\n",
    "\n",
    "    if result.face_landmarks:\n",
    "        for res in result.face_landmarks.landmark:\n",
    "            face.append(np.array([res.x, res.y, res.z]))\n",
    "        face = np.array(face).flatten()\n",
    "    else:\n",
    "        face = np.zeros(468*3)\n",
    "\n",
    "    res = np.concatenate([pose, face, lh, rh])\n",
    "    return res  # Return the final array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path for exported data\n",
    "DATA_PATH = os.path.join('MP_Data')\n",
    "\n",
    "# actions we are going to detect\n",
    "actions = np.array(['hello','yes','no','iloveyou','please','thanks', \"i/me\", \"you\", \"bye\"])\n",
    "\n",
    "# 70 videos worth of data\n",
    "no_sequences = 70\n",
    "\n",
    "# each of thsoe videos are going to be 30 frames in length\n",
    "sequence_length = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(DATA_PATH,action,str(sequence)))\n",
    "        except:\n",
    "            pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "redo_action = \"thanks\"\n",
    "\n",
    "for sequence in range(no_sequences):\n",
    "    try:\n",
    "        os.makedirs(os.path.join(DATA_PATH,redo_action,str(sequence)))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "redo_action = \"you\"\n",
    "\n",
    "for sequence in range(no_sequences):\n",
    "    try:\n",
    "        os.makedirs(os.path.join(DATA_PATH,redo_action,str(sequence)))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cap = cv2.VideoCapture(2)\n",
    "\n",
    "#set the media pipe model\n",
    "# we make a initial detection and then tracks it\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.6) as model:\n",
    "    for action in actions:\n",
    "        for sequence in range(no_sequences):\n",
    "            for frame_num in range(sequence_length):\n",
    "                \n",
    "                # Read feed, grabbing frame\n",
    "                ret, frame = cap.read()\n",
    "            \n",
    "                #making detection\n",
    "                image, results = mediapipe_detection(frame, model)\n",
    "                draw_landmarks(image,results)\n",
    "\n",
    "                if not ret:\n",
    "                    print(\"Failed to read frame\")  \n",
    "                    break  \n",
    "                \n",
    "                if frame_num == 0:\n",
    "                    cv2.putText(image, 'Processing {} video number{}'.format(action,sequence), (15,12),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 1, cv2.LINE_AA)\n",
    "                    cv2.waitKey(300)\n",
    "                else:\n",
    "                    cv2.putText(image,'Collecting frames for {} video number {}'.format(action,sequence),(15,12),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5,(0,0,255),1,cv2.LINE_AA)\n",
    "                  \n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH,action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "                \n",
    "                # Render to screen\n",
    "                cv2.imshow(\"ASL_Interpreter\", image)\n",
    "\n",
    "                # Breaking\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(2)\n",
    "\n",
    "#actions -> (['hello','yes','no','iloveyou','please','thanks', \"i/me\", \"you\", \"bye\"])\n",
    "\n",
    "\n",
    "# to process specific actions, data processing is taking a long time to do in one sitting\n",
    "# now that I am increasing the number of signs to process\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.6) as model:\n",
    "    \n",
    "    # processing specific range of signs to continue processing\n",
    "    for action in actions[8:]:\n",
    "        for sequence in range(no_sequences):\n",
    "            for frame_num in range(sequence_length):\n",
    "                \n",
    "                # Read feed, grabbing frame\n",
    "                ret, frame = cap.read()\n",
    "            \n",
    "                #making detection\n",
    "                image, results = mediapipe_detection(frame, model)\n",
    "                draw_landmarks(image,results)\n",
    "\n",
    "                if not ret:\n",
    "                    print(\"Failed to read frame\")  \n",
    "                    break  \n",
    "                \n",
    "                if frame_num == 0:\n",
    "                    cv2.putText(image, 'Processing {} video number{}'.format(action,sequence), (15,12),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 1, cv2.LINE_AA)\n",
    "                    cv2.waitKey(300)\n",
    "                else:\n",
    "                    cv2.putText(image,'Collecting frames for {} video number {}'.format(action,sequence),(15,12),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5,(0,0,255),1,cv2.LINE_AA)\n",
    "                  \n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH,action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "                \n",
    "                # Render to screen\n",
    "                cv2.imshow(\"ASL_Interpreter\", image)\n",
    "\n",
    "                # Breaking\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asign each label a digit, 0 , 1  , 2 ...\n",
    "label_map = {label:num for num,label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': 0,\n",
       " 'yes': 1,\n",
       " 'no': 2,\n",
       " 'iloveyou': 3,\n",
       " 'please': 4,\n",
       " 'thanks': 5,\n",
       " 'i/me': 6,\n",
       " 'you': 7,\n",
       " 'bye': 8}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating folders for out data, we will have 30 videos per gesture, each video having 30 frames, each frame will be \n",
    "# represented by a npy array that we will train out data on\n",
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        window = []\n",
    "        for frame_num in range(0,sequence_length):\n",
    "            path = os.path.join(DATA_PATH,action, str(sequence), \"{}.npy\".format(frame_num))\n",
    "            res = np.load(path)\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes integer labels from 0,1,2 to 1,0,0 , 0,1,0 , 0,0,1\n",
    "# our labels will be derived from our map, and the videos so we label each video as hello, thanks, or iloveyou\n",
    "Y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 1]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splits our train and test data\n",
    "# 95% of our data will be for training, the 5% being for evaluating our model\n",
    "# we do this because we want to train our model on data , then test it on unseen data to see how well it works\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enabling visualization during training\n",
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing a Sequential model, which lets us add layers step by step\n",
    "# Sequential models are suitable for simple, feed-forward networks where layers are stacked in sequence\n",
    "model = Sequential()\n",
    "\n",
    "# setting return_sequences=True so that this layer will return the full sequence of outputs for each input sequence\n",
    "# using ReLU as the activation function, which helps with vanishing gradient issues by setting negative values to zero\n",
    "# input_shape=(60, 1662) specifies that the input data has sequences of 60 timesteps, each with 1662 features\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(40, 1662)))\n",
    "\n",
    "# again, setting return_sequences=True to output the entire sequence for each input sequence, which allows stacking LSTM layers\n",
    "# this layer will receive the sequence output from the first LSTM layer and further process it to learn more complex patterns\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "\n",
    "# with return_sequences=False, this layer will output only the last timestep, which condenses the sequence into a single output\n",
    "# this allows the model to distill the sequence into a fixed-size vector, useful for passing to dense layers\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "\n",
    "# dense layers are fully connected, meaning each unit in this layer is connected to every unit in the previous layer\n",
    "# the 64 units here enable the model to start learning non-sequential features by combining information from the LSTM layers\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# by adding another dense layer with fewer units, we allow the model to gradually reduce feature complexity\n",
    "# this reduction can help focus on the most important features before reaching the final output layer\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "# the number of units in this layer is equal to the number of actions, defined by actions.shape[0], representing the output classes\n",
    "# softmax activation is used to output a probability distribution across classes, helpful for multi-class classification tasks\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiclass classification model so we have to use categorical accuracy\n",
    "#model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the training :)     \n",
    "model.fit(X_train, Y_train, epochs=2000, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW MODEL <--------- , this model has 98% accuracy, saved as model2.h5\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from keras.layers import TimeDistributed\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(40, 1662)))\n",
    "\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train, epochs=2000, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing dropout and less lstm neurons in second layer\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from keras.layers import TimeDistributed\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(40, 1662)))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train, epochs=2000, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 187ms/step\n"
     ]
    }
   ],
   "source": [
    "res = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'please'"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(res[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'please'"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(Y_test[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 1s 40ms/step\n"
     ]
    }
   ],
   "source": [
    "yhat = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(Y_train, axis=1).tolist()\n",
    "yhat = np.argmax(yhat,axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[535,   0],\n",
       "        [  0,  63]],\n",
       "\n",
       "       [[530,   1],\n",
       "        [  8,  59]],\n",
       "\n",
       "       [[529,   3],\n",
       "        [  8,  58]],\n",
       "\n",
       "       [[520,  13],\n",
       "        [  1,  64]],\n",
       "\n",
       "       [[529,   1],\n",
       "        [  0,  68]],\n",
       "\n",
       "       [[530,   0],\n",
       "        [  1,  67]],\n",
       "\n",
       "       [[531,   0],\n",
       "        [  0,  67]],\n",
       "\n",
       "       [[533,   0],\n",
       "        [  0,  65]],\n",
       "\n",
       "       [[529,   0],\n",
       "        [  0,  69]]], dtype=int64)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing our accuracy, detecting false positives, true positives, false negatives, true nagatives\n",
    "# the more values we have in the top left and bottom right of each matrix the better\n",
    "multilabel_confusion_matrix(ytrue,yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9698996655518395"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# our accuracy of our model from data\n",
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization of the predictions for each action / sign\n",
    "colors = [(25,127,126),(17,24,111),(225,16,45),(25,0,245),(255,233,45),(253,127,126),(253,17,126),(17,2,1),(253,7,126)]\n",
    "\n",
    "def visualize(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    \n",
    "    #starting point and dimensions for the bars\n",
    "    bar_x = 125\n",
    "    bar_height = 30\n",
    "    bar_spacing = 50  # space between each bar\n",
    "    \n",
    "    for num, prob in enumerate(res):\n",
    "        # calc the width based on probability\n",
    "        bar_width = int(prob * 150)  \n",
    "\n",
    "        #  filling rectangle for the probability\n",
    "        start_y = 60 + num * bar_spacing\n",
    "        end_y = start_y + bar_height\n",
    "\n",
    "        print(num)\n",
    "        cv2.rectangle(output_frame, (bar_x, start_y), (bar_x + bar_width, end_y), colors[num], -1)\n",
    "\n",
    "        # drawing white border around each bar\n",
    "        cv2.rectangle(output_frame, (bar_x, start_y), (bar_x + bar_width, end_y), (255, 255, 255), 1)\n",
    "\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 0.7\n",
    "        thickness = 2\n",
    "    \n",
    "        action_text = actions[num]\n",
    "        action_position = (10, end_y - 5)\n",
    "\n",
    "        \n",
    "        cv2.putText(output_frame, action_text, action_position, font, font_scale, (255, 255, 255), thickness, cv2.LINE_AA)\n",
    "\n",
    "        percentage_text = f\"{int(prob * 100)}%\"\n",
    "        percent_position = (bar_x + bar_width + 10, end_y - 5)\n",
    "\n",
    "    \n",
    "        cv2.putText(output_frame, percentage_text, percent_position, font, font_scale, (255, 255, 255), thickness, cv2.LINE_AA)\n",
    "    \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#detection vars\n",
    "sequence = []\n",
    "predictions = []\n",
    "\n",
    "cap = cv2.VideoCapture(2)\n",
    "\n",
    "\n",
    "\n",
    "predicting = False\n",
    "\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to read frame\")  \n",
    "            break  \n",
    "\n",
    "        # Making detection\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_landmarks(image, results)\n",
    "        \n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-20:]\n",
    "\n",
    "        if len(sequence) == 20:\n",
    "            predicting = True\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            predictions.append(np.argmax(res))\n",
    "        else:\n",
    "            predicting = False\n",
    "\n",
    "        if predicting:\n",
    "            image = visualize(res, actions, image, colors)\n",
    "\n",
    "        cv2.imshow('ASL_Interpreter', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
